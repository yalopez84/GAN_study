{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yalopez84/GAN_study/blob/master/Estudiando_KBGAN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KTch7ZJ4SHYH"
      },
      "outputs": [],
      "source": [
        "#Libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "from torch.optim import Adam, SGD, Adagrad\n",
        "from torch.autograd import Variable\n",
        "from random import randint\n",
        "from collections import defaultdict\n",
        "from numpy.random import choice, randint\n",
        "import numpy as n\n",
        "import datetime\n",
        "import yaml\n",
        "import sys\n",
        "import logging\n",
        "import subprocess\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU5XnYitQYvg",
        "outputId": "2cd54f2a-829b-4b51-acf2-fd59112c5be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device cuda\n"
          ]
        }
      ],
      "source": [
        "#Directories and devices\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir=\"/content/drive/MyDrive/NegativeStrategies/OAGAN-NS/data/\"\n",
        "os.chdir(data_dir)\n",
        "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "print(\"Device\",device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "iok96m3hTzko"
      },
      "outputs": [],
      "source": [
        "#base_model.py\n",
        "class BaseModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseModule,self).__init__()\n",
        "    def score(self,src,rel,dst):\n",
        "        raise NotImplementedError\n",
        "    def dist(self,src,rel,dst):\n",
        "        raise NotImplementedError\n",
        "    def prob_logit(self,src,rel,dst):\n",
        "        raise NotImplementedError\n",
        "    def prob(self,src,rel,dst):\n",
        "        return f.softmax(self.prob_logit(src,rel,dst))\n",
        "    def constraint(self):\n",
        "        pass\n",
        "    def pair_loss(self,src,rel,dst, src_bad,dst_bad):\n",
        "        d_good=self.dist(src,rel,dst)\n",
        "        d_bad=self.dist(src_bad,rel,dst_bad)\n",
        "        return f.relu(self.margin + d_good - d_bad)\n",
        "\n",
        "    def softmax_loss(self, src, rel, dst, truth):\n",
        "        probs=self.prob(src,rel,dst)\n",
        "        n=probs.size(0)\n",
        "        truth_probs=torch.log(probs[torch.arange(0,n).type(torch.LongTensor).cuda(),truth]+1e-30)\n",
        "        return -truth_probs\n",
        "\n",
        "class BaseModel(object):\n",
        "    def __init__(self):\n",
        "        self.mdl= None\n",
        "        self.weight_decay = 0\n",
        "    def save(self,filename):\n",
        "        torch.save(self.mdl.state_dict(),filename)\n",
        "\n",
        "    def load(self,filename):\n",
        "        self.mdl.load_state_dict(torch.load(filename,map_location=lambda storage, location:storage.cuda()))\n",
        "\n",
        "    def gen_step(self,src,rel,dst,n_sample=1,temperature=1.0,train=True):\n",
        "        pdb.set_trace()\n",
        "        if not hasattr(self,'opt'):\n",
        "            self.opt=Adam(self.mdl.parameters(), weight_decay=self.weight_decay)\n",
        "        _n,_m=dst.size()\n",
        "        rel_var=Variable(rel.cuda())\n",
        "        src_var = Variable(src.cuda())\n",
        "        dst_var = Variable(dst.cuda())\n",
        "        logits=self.mdl.prob_logit(src_var, rel_var, dst_var)/temperature\n",
        "        probs=f.softmax(logits)\n",
        "        row_idx=torch.arange(0,_n).type(torch.LongTensor).unsqueeze(1).expand(_n, n_sample)\n",
        "        sample_idx=torch.multinomial(probs,n_sample, replacement=True)\n",
        "        sample_srcs = src[row_idx, sample_idx.data.cpu()]\n",
        "        sample_dsts = dst[row_idx, sample_idx.data.cpu()]\n",
        "        rewards = yield sample_srcs, sample_dsts\n",
        "        pdb.set_trace()\n",
        "        if train:\n",
        "            self.mdl.zero_grad()\n",
        "            log_probs = f.log_softmax(logits)\n",
        "            reinforce_loss = -torch.sum(Variable(rewards) * log_probs[row_idx.cuda(), sample_idx.data])\n",
        "            reinforce_loss.backward()\n",
        "            self.opt.step()\n",
        "            self.mdl.constraint()\n",
        "        yield None\n",
        "\n",
        "    def dis_step(self,src, rel, dst, src_fake, dst_fake, train=True):\n",
        "        pdb.set_trace()\n",
        "        if not hasattr(self,'opt'):\n",
        "            self.opt = Adam(self.mdl.parameters(), weight_decay=self.weight_decay)\n",
        "        src_var = Variable(src.cuda())\n",
        "        rel_var = Variable(rel.cuda())\n",
        "        dst_var = Variable(dst.cuda())\n",
        "        src_fake_var = Variable(src_fake.cuda())\n",
        "        dst_fake_var = Variable(dst_fake.cuda())\n",
        "        losses = self.mdl.pair_loss(src_var, rel_var, dst_var, src_fake_var, dst_fake_var)\n",
        "        fake_scores = self.mdl.score(src_fake_var, rel_var, dst_fake_var)\n",
        "        if train:\n",
        "            self.mdl.zero_grad()\n",
        "            torch.sum(losses).backward()\n",
        "            self.opt.step()\n",
        "            self.mdl.constraint()\n",
        "        return losses.data, -fake_scores.data\n",
        "\n",
        "    def test_link(self, test_data, n_ent, heads, tails, filt=False):\n",
        "\n",
        "        mrr_tot = 0\n",
        "        mr_tot = 0\n",
        "        hit10_tot = 0\n",
        "        count = 0\n",
        "        for batch_s, batch_r, batch_t in batch_by_size(config().test_batch_size, *test_data):\n",
        "            batch_size = batch_s.size(0)\n",
        "            rel_var = Variable(batch_r.unsqueeze(1).expand(batch_size, n_ent).cuda())\n",
        "            src_var = Variable(batch_s.unsqueeze(1).expand(batch_size, n_ent).cuda())\n",
        "            dst_var = Variable(batch_t.unsqueeze(1).expand(batch_size, n_ent).cuda())\n",
        "            all_var = Variable(torch.arange(0, n_ent).unsqueeze(0).expand(batch_size, n_ent)\n",
        "                               .type(torch.LongTensor).cuda(), volatile=True)\n",
        "            with torch.no_grad():\n",
        "                batch_dst_scores = self.mdl.score(src_var, rel_var, all_var).data\n",
        "                batch_src_scores = self.mdl.score(all_var, rel_var, dst_var).data\n",
        "\n",
        "            for ss, rr, tt, dst_scores, src_scores in zip(batch_s, batch_r, batch_t, batch_dst_scores, batch_src_scores):\n",
        "\n",
        "                if filt:\n",
        "                    if tails[(ss.item(), rr.item())]._nnz() > 1:\n",
        "                        tmp = dst_scores[tt]\n",
        "                        dst_scores += tails[(ss.item(), rr.item())].cuda() * 1e30\n",
        "                        dst_scores[tt] = tmp\n",
        "\n",
        "                    if heads[(tt.item(), rr.item())]._nnz() > 1:\n",
        "                        tmp = src_scores[ss]\n",
        "                        src_scores += heads[(tt.item(), rr.item())].cuda() * 1e30\n",
        "                        src_scores[ss] = tmp\n",
        "                mrr, mr, hit10 =mrr_mr_hitk(dst_scores, tt)\n",
        "                mrr_tot += mrr\n",
        "                mr_tot += mr\n",
        "                hit10_tot += hit10\n",
        "                mrr, mr, hit10 = mrr_mr_hitk(src_scores, ss)\n",
        "                mrr_tot += mrr\n",
        "                mr_tot += mr\n",
        "                hit10_tot += hit10\n",
        "                count += 2\n",
        "        logging.info('Test_MRR=%f, Test_MR=%f, Test_H@10=%f', mrr_tot / count, mr_tot / count, hit10_tot / count)\n",
        "        print('La suma de los mrr es: ', mrr_tot, 'la cantidad de mrr calculados en el test es de:', count)\n",
        "        return mrr_tot / count\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "U9-d4Nx7RmKR"
      },
      "outputs": [],
      "source": [
        "#data_utils.py\n",
        "def heads_tails(n_ent, train_data, valid_data=None, test_data=None):\n",
        "    train_src, train_rel, train_dst = train_data\n",
        "    if valid_data:\n",
        "        valid_src, valid_rel, valid_dst = valid_data\n",
        "    else:\n",
        "        valid_src = valid_rel = valid_dst = []\n",
        "    if test_data:\n",
        "        test_src, test_rel, test_dst = test_data\n",
        "    else:\n",
        "        test_src = test_rel = test_dst = []\n",
        "    all_src = train_src + valid_src + test_src\n",
        "    all_rel = train_rel + valid_rel + test_rel\n",
        "    all_dst = train_dst + valid_dst + test_dst\n",
        "    heads = defaultdict(lambda: set())\n",
        "    tails = defaultdict(lambda: set())\n",
        "    for ss, rr, tt in zip(all_src, all_rel, all_dst):\n",
        "        tails[(ss, rr)].add(tt)\n",
        "        heads[(tt, rr)].add(ss)\n",
        "    heads_sp = {}\n",
        "    tails_sp = {}\n",
        "    for k in tails.keys():\n",
        "        tails_sp[k] = torch.sparse.FloatTensor(torch.LongTensor([list(tails[k])]),\n",
        "                                               torch.ones(len(tails[k])), torch.Size([n_ent]))\n",
        "    for k in heads.keys():\n",
        "        heads_sp[k] = torch.sparse.FloatTensor(torch.LongTensor([list(heads[k])]),\n",
        "                                               torch.ones(len(heads[k])), torch.Size([n_ent]))\n",
        "    return heads_sp, tails_sp\n",
        "\n",
        "\n",
        "def inplace_shuffle(*lists):\n",
        "    idx = []\n",
        "    for i in range(len(lists[0])):\n",
        "        a=i\n",
        "        if a==0:\n",
        "            a=1\n",
        "        idx.append(randint(0, a))\n",
        "    for ls in lists:\n",
        "        for i, item in enumerate(ls):\n",
        "            j = idx[i]\n",
        "            ls[i], ls[j] = ls[j], ls[i]\n",
        "\n",
        "\n",
        "def batch_by_num(n_batch, *lists, n_sample=None):\n",
        "    if n_sample is None:\n",
        "        n_sample = len(lists[0])\n",
        "    for i in range(n_batch):\n",
        "        head = int(n_sample * i / n_batch)\n",
        "        tail = int(n_sample * (i + 1) / n_batch)\n",
        "        ret = [ls[head:tail] for ls in lists]\n",
        "        if len(ret) > 1:\n",
        "            yield ret\n",
        "        else:\n",
        "            yield ret[0]\n",
        "\n",
        "def batch_by_size(batch_size, *lists, n_sample=None):\n",
        "    if n_sample is None:\n",
        "        n_sample = len(lists[0])\n",
        "    head = 0\n",
        "    while head < n_sample:\n",
        "        tail = min(n_sample, head + batch_size)\n",
        "        ret = [ls[head:tail] for ls in lists]\n",
        "        head += batch_size\n",
        "        if len(ret) > 1:\n",
        "            yield ret\n",
        "        else:\n",
        "            yield ret[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BKH-NlZYS9ox"
      },
      "outputs": [],
      "source": [
        "#config.py\n",
        "class ConfigDict(dict):\n",
        "    __getattr__ = dict.__getitem__\n",
        "\n",
        "def _make_config_dict(obj):\n",
        "\n",
        "    if isinstance(obj, dict):\n",
        "        return ConfigDict({k: _make_config_dict(v) for k, v in obj.items()})\n",
        "    elif isinstance(obj, list):\n",
        "        return [_make_config_dict(x) for x in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "_config = None\n",
        "\n",
        "def config():\n",
        "    arg_dict ={\n",
        "        '--config':'config_fb15k237.yaml',\n",
        "        '--pretrain_config':'<model_name>'\n",
        "    }\n",
        "    global _config\n",
        "    if _config is None:\n",
        "        config_path='config_fb15k237.yaml'\n",
        "        with open(os.path.join(data_dir, config_path)) as f:\n",
        "            _config = _make_config_dict(yaml.full_load(f))\n",
        "    return _config\n",
        "\n",
        "def _dump_config(obj, prefix):\n",
        "    if isinstance(obj, dict):\n",
        "        for k, v in obj.items():\n",
        "            _dump_config(v, prefix + (k,))\n",
        "    elif isinstance(obj, list):\n",
        "        for i, v in enumerate(obj):\n",
        "            _dump_config(v, prefix + (str(i),))\n",
        "    else:\n",
        "        if isinstance(obj, str):\n",
        "            rep = obj\n",
        "        else:\n",
        "            rep = repr(obj)\n",
        "        logging.debug('%s=%s', '.'.join(prefix), rep)\n",
        "\n",
        "def dump_config():\n",
        "    return _dump_config(_config, tuple())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Mnasg894VZJs"
      },
      "outputs": [],
      "source": [
        "#metrics.py\n",
        "def mrr_mr_hitk(scores, target, k=10):\n",
        "    values, sorted_idx = torch.sort(scores)\n",
        "    find_target = sorted_idx == target\n",
        "    target_rank = torch.nonzero(find_target)[0, 0] + 1\n",
        "    return 1 / target_rank, target_rank, int(target_rank <= k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "6vTg8ojpcZkT"
      },
      "outputs": [],
      "source": [
        "#corrupter.py\n",
        "def get_bern_prob(data, n_ent, n_rel):\n",
        "    src, rel, dst = data\n",
        "    edges = defaultdict(lambda: defaultdict(lambda: set()))\n",
        "    rev_edges = defaultdict(lambda: defaultdict(lambda: set()))\n",
        "    for ss, rr, tt in zip(src, rel, dst):\n",
        "        edges[rr.item()][ss.item()].add(tt.item())\n",
        "        rev_edges[rr.item()][tt.item()].add(ss.item())\n",
        "    bern_prob = torch.zeros(n_rel)\n",
        "    for rrr in edges.keys():\n",
        "        tph = sum(len(tails) for tails in edges[rrr].values()) / len(edges[rrr])\n",
        "        htp = sum(len(heads) for heads in rev_edges[rrr].values()) / len(rev_edges[rrr])\n",
        "        bern_prob[rrr] = tph / (tph + htp)\n",
        "    return bern_prob\n",
        "\n",
        "class BernCorrupter(object):\n",
        "    def __init__(self, data, n_ent, n_rel):\n",
        "        self.bern_prob = get_bern_prob(data, n_ent, n_rel)\n",
        "        self.n_ent = n_ent\n",
        "\n",
        "    def corrupt(self, src, rel, dst):\n",
        "        prob = self.bern_prob[rel]\n",
        "        selection = torch.bernoulli(prob).numpy().astype('int64')\n",
        "        ent_random = choice(self.n_ent, len(src))\n",
        "        src_out = (1 - selection) * src.numpy() + selection * ent_random\n",
        "        dst_out = selection * dst.numpy() + (1 - selection) * ent_random\n",
        "        return torch.from_numpy(src_out), torch.from_numpy(dst_out)\n",
        "\n",
        "class BernCorrupterMulti(object):\n",
        "    def __init__(self, data, n_ent, n_rel, n_sample):\n",
        "        self.bern_prob = get_bern_prob(data, n_ent, n_rel)\n",
        "        self.n_ent = n_ent\n",
        "        self.n_sample = n_sample\n",
        "\n",
        "    def corrupt(self, src, rel, dst, keep_truth=True):\n",
        "        nn = len(src)\n",
        "        prob = self.bern_prob[rel]\n",
        "        selection = torch.bernoulli(prob).numpy().astype('bool')\n",
        "        src_out = n.tile(src.numpy(), (self.n_sample, 1)).transpose()\n",
        "        dst_out = n.tile(dst.numpy(), (self.n_sample, 1)).transpose()\n",
        "        rel_out = rel.unsqueeze(1).expand(nn, self.n_sample)\n",
        "        if keep_truth:\n",
        "            ent_random = choice(self.n_ent, (nn, self.n_sample - 1))\n",
        "            src_out[selection, 1:] = ent_random[selection]\n",
        "            dst_out[~selection, 1:] = ent_random[~selection]\n",
        "        else:\n",
        "            ent_random = choice(self.n_ent, (nn, self.n_sample))\n",
        "            src_out[selection, :] = ent_random[selection]\n",
        "            dst_out[~selection, :] = ent_random[~selection]\n",
        "        return torch.from_numpy(src_out), rel_out, torch.from_numpy(dst_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "OgZcjk8DLwYo"
      },
      "outputs": [],
      "source": [
        "#read_data.py\n",
        "KBIndex = namedtuple('KBIndex', ['ent_list', 'rel_list', 'ent_id', 'rel_id'])\n",
        "\n",
        "def index_ent_rel(*filenames):\n",
        "    ent_set = set()\n",
        "    rel_set = set()\n",
        "    for filename in filenames:\n",
        "        with open(filename) as f:\n",
        "            for ln in f:\n",
        "                s, r, t = ln.strip().split('\\t')[:3]\n",
        "                ent_set.add(s)\n",
        "                ent_set.add(t)\n",
        "                rel_set.add(r)\n",
        "    ent_list = sorted(list(ent_set))\n",
        "    rel_list = sorted(list(rel_set))\n",
        "    ent_id = dict(zip(ent_list, count()))\n",
        "    rel_id = dict(zip(rel_list, count()))\n",
        "    return KBIndex(ent_list, rel_list, ent_id, rel_id)\n",
        "\n",
        "\n",
        "def graph_size(kb_index):\n",
        "    return len(kb_index.ent_id), len(kb_index.rel_id)\n",
        "\n",
        "\n",
        "def read_data(filename, kb_index):\n",
        "    src = []\n",
        "    rel = []\n",
        "    dst = []\n",
        "    with open(filename) as f:\n",
        "        for ln in f:\n",
        "            s, r, t = ln.strip().split('\\t')\n",
        "            src.append(kb_index.ent_id[s])\n",
        "            rel.append(kb_index.rel_id[r])\n",
        "            dst.append(kb_index.ent_id[t])\n",
        "    return src, rel, dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CRDCvDPdIKWe"
      },
      "outputs": [],
      "source": [
        "#trans_e.py\n",
        "class TransEModule(BaseModule):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(TransEModule, self).__init__()\n",
        "        self.p = config.p\n",
        "        self.margin = config.margin\n",
        "        self.temp = config.get('temp', 1)\n",
        "        self.rel_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.ent_embed = nn.Embedding(n_ent, config.dim)\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.normal_(1 / param.size(1) ** 0.5)\n",
        "            param.data.renorm_(2, 0, 1)\n",
        "\n",
        "    def forward(self, src, rel, dst):\n",
        "        return torch.norm(self.ent_embed(dst) - self.ent_embed(src) - self.rel_embed(rel) + 1e-30, p=self.p, dim=-1)\n",
        "\n",
        "    def dist(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "    def score(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "    def prob_logit(self, src, rel, dst):\n",
        "        return -self.forward(src, rel ,dst) / self.temp\n",
        "\n",
        "    def constraint(self):\n",
        "        self.ent_embed.weight.data.renorm_(2, 0, 1)\n",
        "        self.rel_embed.weight.data.renorm_(2, 0, 1)\n",
        "\n",
        "class TransE(BaseModel):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(TransE, self).__init__()\n",
        "        self.mdl = TransEModule(n_ent, n_rel, config)\n",
        "        self.mdl.cuda()\n",
        "        self.config = config\n",
        "\n",
        "    def pretrain(self, train_data, corrupter, tester):\n",
        "\n",
        "        src, rel, dst = train_data\n",
        "        n_train = len(src)\n",
        "        optimizer = Adam(self.mdl.parameters())\n",
        "        #optimizer = SGD(self.mdl.parameters(), lr=1e-4)\n",
        "        n_epoch = self.config.n_epoch\n",
        "        n_batch = self.config.n_batch\n",
        "        best_perf = 0\n",
        "        for epoch in range(n_epoch):\n",
        "            print('---------epoch_number---', epoch)\n",
        "            epoch_loss = 0\n",
        "            rand_idx = torch.randperm(n_train)\n",
        "            src = src[rand_idx]\n",
        "            rel = rel[rand_idx]\n",
        "            dst = dst[rand_idx]\n",
        "            src_corrupted, dst_corrupted = corrupter.corrupt(src, rel, dst)\n",
        "            src_cuda = src.cuda()\n",
        "            rel_cuda = rel.cuda()\n",
        "            dst_cuda = dst.cuda()\n",
        "            src_corrupted = src_corrupted.cuda()\n",
        "            dst_corrupted = dst_corrupted.cuda()\n",
        "            for s0, r, t0, s1, t1 in batch_by_num(n_batch, src_cuda, rel_cuda, dst_cuda, src_corrupted, dst_corrupted,\n",
        "                                                  n_sample=n_train):\n",
        "\n",
        "                self.mdl.zero_grad()\n",
        "                loss = torch.sum(self.mdl.pair_loss(Variable(s0), Variable(r), Variable(t0), Variable(s1), Variable(t1)))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                self.mdl.constraint()\n",
        "                epoch_loss += loss.item()\n",
        "                #print('epoch_loss', epoch_loss)\n",
        "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
        "            if (epoch + 1) % self.config.epoch_per_test == 0:\n",
        "                test_perf = tester()\n",
        "                print('El MRR en epoch ',epoch,'fue de :', test_perf)\n",
        "                if test_perf > best_perf:\n",
        "                    task_dir = config().task.dir\n",
        "                    direccion=os.path.join(data_dir, task_dir)\n",
        "                    self.save(os.path.join(direccion, self.config.model_file))\n",
        "                    best_perf = test_perf\n",
        "        return best_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_lPB_Xuizcf4"
      },
      "outputs": [],
      "source": [
        "#trans_d.py\n",
        "class TransDModule(BaseModule):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(TransDModule, self).__init__()\n",
        "        self.margin = config.margin\n",
        "        self.p = config.p\n",
        "        self.temp = config.get('temp', 1)\n",
        "        self.rel_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.ent_embed = nn.Embedding(n_ent, config.dim)\n",
        "        self.proj_rel_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.proj_ent_embed = nn.Embedding(n_ent, config.dim)\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.normal_(1 / param.size(1) ** 0.5)\n",
        "            param.data.renorm_(2, 0, 1)\n",
        "\n",
        "    def forward(self, src, rel, dst):\n",
        "        src_proj = self.ent_embed(src) +\\\n",
        "                   torch.sum(self.proj_ent_embed(src) * self.ent_embed(src), dim=-1, keepdim=True) * self.proj_rel_embed(rel)\n",
        "        dst_proj = self.ent_embed(dst) +\\\n",
        "                   torch.sum(self.proj_ent_embed(dst) * self.ent_embed(dst), dim=-1, keepdim=True) * self.proj_rel_embed(rel)\n",
        "        return torch.norm(dst_proj - self.rel_embed(rel) - src_proj + 1e-30, p=self.p, dim=-1)\n",
        "\n",
        "    def dist(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "    def score(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "    def prob_logit(self, src, rel, dst):\n",
        "        return -self.forward(src, rel ,dst) / self.temp\n",
        "\n",
        "    def constraint(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.renorm_(2, 0, 1)\n",
        "\n",
        "class TransD(BaseModel):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(TransD, self).__init__()\n",
        "        self.mdl = TransDModule(n_ent, n_rel, config)\n",
        "        self.mdl.cuda()\n",
        "        self.config = config\n",
        "\n",
        "    def pretrain(self, train_data, corrupter, tester):\n",
        "        print('estoy aqui en el pretrain de TransD')\n",
        "        src, rel, dst = train_data\n",
        "        n_train = len(src)\n",
        "        optimizer = Adam(self.mdl.parameters())\n",
        "        #optimizer = SGD(self.mdl.parameters(), lr=1e-4)\n",
        "        n_epoch = self.config.n_epoch\n",
        "        n_batch = self.config.n_batch\n",
        "        best_perf = 0\n",
        "        for epoch in range(n_epoch):\n",
        "            print('---------epoch_number---', epoch)\n",
        "            epoch_loss = 0\n",
        "            rand_idx = torch.randperm(n_train)\n",
        "            src = src[rand_idx]\n",
        "            rel = rel[rand_idx]\n",
        "            dst = dst[rand_idx]\n",
        "            src_corrupted, dst_corrupted = corrupter.corrupt(src, rel, dst)\n",
        "            src_cuda = src.cuda()\n",
        "            rel_cuda = rel.cuda()\n",
        "            dst_cuda = dst.cuda()\n",
        "            src_corrupted = src_corrupted.cuda()\n",
        "            dst_corrupted = dst_corrupted.cuda()\n",
        "            for s0, r, t0, s1, t1 in batch_by_num(n_batch, src_cuda, rel_cuda, dst_cuda, src_corrupted, dst_corrupted,\n",
        "                                                  n_sample=n_train):\n",
        "                self.mdl.zero_grad()\n",
        "                loss = torch.sum(self.mdl.pair_loss(Variable(s0), Variable(r), Variable(t0), Variable(s1), Variable(t1)))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                self.mdl.constraint()\n",
        "                epoch_loss += loss.item()\n",
        "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
        "            if (epoch + 1) % self.config.epoch_per_test == 0:\n",
        "                test_perf = tester()\n",
        "                print('El MRR en epoch ',epoch,'fue de :', test_perf)\n",
        "                if test_perf > best_perf:\n",
        "                    task_dir = config().task.dir\n",
        "                    direccion=os.path.join(data_dir, task_dir)\n",
        "                    self.save(os.path.join(direccion, self.config.model_file))\n",
        "                    best_perf = test_perf\n",
        "        return best_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "iMsrsJJ2HF9f"
      },
      "outputs": [],
      "source": [
        "#distmult.py\n",
        "class DistMultModule(BaseModule):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(DistMultModule, self).__init__()\n",
        "        sigma = 0.2\n",
        "        self.rel_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.rel_embed.weight.data.div_((config.dim / sigma ** 2) ** (1 / 6))\n",
        "        self.ent_embed = nn.Embedding(n_ent, config.dim)\n",
        "        self.ent_embed.weight.data.div_((config.dim / sigma ** 2) ** (1 / 6))\n",
        "\n",
        "    def forward(self, src, rel, dst):\n",
        "        return torch.sum(self.ent_embed(dst) * self.ent_embed(src) * self.rel_embed(rel), dim=-1)\n",
        "\n",
        "    def score(self, src, rel, dst):\n",
        "        return -self.forward(src, rel, dst)\n",
        "\n",
        "    def dist(self, src, rel, dst):\n",
        "        return -self.forward(src, rel, dst)\n",
        "\n",
        "    def prob_logit(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "class DistMult(BaseModel):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(DistMult, self).__init__()\n",
        "        self.mdl = DistMultModule(n_ent, n_rel, config)\n",
        "        self.mdl.cuda()\n",
        "        self.config = config\n",
        "        self.weight_decay = config.lam / config.n_batch\n",
        "\n",
        "    def pretrain(self, train_data, corrupter, tester):\n",
        "        src, rel, dst = train_data\n",
        "        n_train = len(src)\n",
        "        n_epoch = self.config.n_epoch\n",
        "        n_batch = self.config.n_batch\n",
        "        optimizer = Adam(self.mdl.parameters(), weight_decay=self.weight_decay)\n",
        "        best_perf = 0\n",
        "        for epoch in range(n_epoch):\n",
        "            print('---------epoch_number---', epoch)\n",
        "            epoch_loss = 0\n",
        "            if epoch % self.config.sample_freq == 0:\n",
        "                rand_idx = torch.randperm(n_train)\n",
        "                src = src[rand_idx]\n",
        "                rel = rel[rand_idx]\n",
        "                dst = dst[rand_idx]\n",
        "                src_corrupted, rel_corrupted, dst_corrupted = corrupter.corrupt(src, rel, dst)\n",
        "                src_corrupted = src_corrupted.cuda()\n",
        "                rel_corrupted = rel_corrupted.cuda()\n",
        "                dst_corrupted = dst_corrupted.cuda()\n",
        "            for ss, rs, ts in batch_by_num(n_batch, src_corrupted, rel_corrupted, dst_corrupted, n_sample=n_train):\n",
        "                self.mdl.zero_grad()\n",
        "                label = torch.zeros(len(ss)).type(torch.LongTensor).cuda()\n",
        "                loss = torch.sum(self.mdl.softmax_loss(Variable(ss), Variable(rs), Variable(ts), label))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
        "            if (epoch + 1) % self.config.epoch_per_test == 0:\n",
        "                test_perf = tester()\n",
        "                print('El MRR en epoch ',epoch,'fue de :', test_perf)\n",
        "                if test_perf > best_perf:\n",
        "                    task_dir = config().task.dir\n",
        "                    direccion=os.path.join(data_dir, task_dir)\n",
        "                    self.save(os.path.join(direccion, self.config.model_file))\n",
        "                    best_perf = test_perf\n",
        "        return best_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "III29HhjpiL3"
      },
      "outputs": [],
      "source": [
        "class ComplExModule(BaseModule):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(ComplExModule, self).__init__()\n",
        "        sigma = 0.2\n",
        "        self.rel_re_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.rel_im_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.ent_re_embed = nn.Embedding(n_ent, config.dim)\n",
        "        self.ent_im_embed = nn.Embedding(n_ent, config.dim)\n",
        "        for param in self.parameters():\n",
        "            param.data.div_((config.dim / sigma ** 2) ** (1 / 6))\n",
        "\n",
        "    def forward(self, src, rel, dst):\n",
        "        return torch.sum(self.rel_re_embed(rel) * self.ent_re_embed(src) * self.ent_re_embed(dst), dim=-1) \\\n",
        "            + torch.sum(self.rel_re_embed(rel) * self.ent_im_embed(src) * self.ent_im_embed(dst), dim=-1) \\\n",
        "            + torch.sum(self.rel_im_embed(rel) * self.ent_re_embed(src) * self.ent_im_embed(dst), dim=-1) \\\n",
        "            - torch.sum(self.rel_im_embed(rel) * self.ent_im_embed(src) * self.ent_re_embed(dst), dim=-1)\n",
        "\n",
        "    def score(self, src, rel, dst):\n",
        "        return -self.forward(src, rel, dst)\n",
        "\n",
        "    def dist(self, src, rel, dst):\n",
        "        return -self.forward(src, rel, dst)\n",
        "\n",
        "    def prob_logit(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "class ComplEx(BaseModel):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(ComplEx, self).__init__()\n",
        "        self.mdl = ComplExModule(n_ent, n_rel, config)\n",
        "        self.mdl.cuda()\n",
        "        self.config = config\n",
        "        self.weight_decay = config.lam / config.n_batch\n",
        "\n",
        "    def pretrain(self, train_data, corrupter, tester):\n",
        "        src, rel, dst = train_data\n",
        "        n_train = len(src)\n",
        "        n_epoch = self.config.n_epoch\n",
        "        n_batch = self.config.n_batch\n",
        "        optimizer = Adam(self.mdl.parameters(), weight_decay=self.weight_decay)\n",
        "        best_perf = 0\n",
        "        for epoch in range(n_epoch):\n",
        "            print('---------epoch_number---', epoch)\n",
        "            epoch_loss = 0\n",
        "            if epoch % self.config.sample_freq == 0:\n",
        "                rand_idx = torch.randperm(n_train)\n",
        "                src = src[rand_idx]\n",
        "                rel = rel[rand_idx]\n",
        "                dst = dst[rand_idx]\n",
        "                src_corrupted, rel_corrupted, dst_corrupted = corrupter.corrupt(src, rel, dst)\n",
        "                src_corrupted = src_corrupted.cuda()\n",
        "                rel_corrupted = rel_corrupted.cuda()\n",
        "                dst_corrupted = dst_corrupted.cuda()\n",
        "            for ss, rs, ts in batch_by_num(n_batch, src_corrupted, rel_corrupted, dst_corrupted, n_sample=n_train):\n",
        "                self.mdl.zero_grad()\n",
        "                label = torch.zeros(len(ss)).type(torch.LongTensor).cuda()\n",
        "                loss = torch.sum(self.mdl.softmax_loss(Variable(ss), Variable(rs), Variable(ts), label))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
        "            if (epoch + 1) % self.config.epoch_per_test == 0:\n",
        "                test_perf = tester()\n",
        "                print('El MRR en epoch ',epoch,'fue de :', test_perf)\n",
        "                if test_perf > best_perf:\n",
        "                    task_dir = config().task.dir\n",
        "                    direccion=os.path.join(data_dir, task_dir)\n",
        "                    self.save(os.path.join(direccion, self.config.model_file))\n",
        "                    best_perf = test_perf\n",
        "        return best_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "OsOL03pqC_cF"
      },
      "outputs": [],
      "source": [
        "#pretrain.py\n",
        "def pretrain():\n",
        "    task_dir = config().task.dir\n",
        "    direccion=os.path.join(data_dir, task_dir)\n",
        "    kb_index = index_ent_rel(os.path.join(direccion, 'train.txt'),\n",
        "                        os.path.join(direccion, 'valid.txt'),\n",
        "                        os.path.join(direccion, 'test.txt'))\n",
        "\n",
        "    n_ent, n_rel = graph_size(kb_index)\n",
        "    train_data = read_data(os.path.join(direccion, 'train.txt'), kb_index)\n",
        "    inplace_shuffle(*train_data)\n",
        "    valid_data = read_data(os.path.join(direccion, 'valid.txt'), kb_index)\n",
        "    test_data = read_data(os.path.join(direccion, 'test.txt'), kb_index)\n",
        "\n",
        "    heads, tails = heads_tails(n_ent, train_data, valid_data, test_data)\n",
        "    valid_data = [torch.LongTensor(vec) for vec in valid_data]\n",
        "    test_data = [torch.LongTensor(vec) for vec in test_data]\n",
        "    tester = lambda: gen.test_link(valid_data, n_ent, heads, tails)\n",
        "    train_data = [torch.LongTensor(vec) for vec in train_data]\n",
        "    mdl_type = config().pretrain_config\n",
        "    gen_config = config()[mdl_type]\n",
        "    if mdl_type == 'TransE':\n",
        "        corrupter = BernCorrupter(train_data, n_ent, n_rel)\n",
        "        gen = TransE(n_ent, n_rel, gen_config)\n",
        "    elif mdl_type == 'TransD':\n",
        "        corrupter = BernCorrupter(train_data, n_ent, n_rel)\n",
        "        gen = TransD(n_ent, n_rel, gen_config)\n",
        "    elif mdl_type == 'DistMult':\n",
        "        corrupter = BernCorrupterMulti(train_data, n_ent, n_rel, gen_config.n_sample)\n",
        "        gen = DistMult(n_ent, n_rel, gen_config)\n",
        "    elif mdl_type == 'ComplEx':\n",
        "        corrupter = BernCorrupterMulti(train_data, n_ent, n_rel, gen_config.n_sample)\n",
        "        gen = ComplEx(n_ent, n_rel, gen_config)\n",
        "    print(\"Comenzando el entrenamiento con el modelo:\", mdl_type)\n",
        "    result=gen.pretrain(train_data, corrupter, tester)\n",
        "    print('El best MRR fue de :', result)\n",
        "    print('Cargando modelo preentrenado')\n",
        "    gen.load(os.path.join(direccion, gen_config.model_file))\n",
        "    print('Ejecutando pruebas')\n",
        "    result=gen.test_link(test_data, n_ent, heads, tails)\n",
        "    print('El MRR para el conjunto de pruebas en el modelo entrenado con :', gen_config.n_epoch,'epochs es de: ',result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3dneGeqpVDA0"
      },
      "outputs": [],
      "source": [
        "#gan_train.py\n",
        "def gan_train():\n",
        "    task_dir = config().task.dir\n",
        "    direccion=os.path.join(data_dir, task_dir)\n",
        "    kb_index = index_ent_rel(os.path.join(direccion, 'train.txt'),\n",
        "                             os.path.join(direccion, 'valid.txt'),\n",
        "                             os.path.join(direccion, 'test.txt'))\n",
        "    n_ent, n_rel = graph_size(kb_index)\n",
        "\n",
        "\n",
        "    models = {'TransE': TransE, 'TransD': TransD, 'DistMult': DistMult, 'ComplEx': ComplEx}\n",
        "    gen_config = config()[config().g_config]\n",
        "    dis_config = config()[config().d_config]\n",
        "    gen = models[config().g_config](n_ent, n_rel, gen_config)\n",
        "    dis = models[config().d_config](n_ent, n_rel, dis_config)\n",
        "    gen.load(os.path.join(direccion, gen_config.model_file))\n",
        "    dis.load(os.path.join(direccion, dis_config.model_file))\n",
        "    train_data = read_data(os.path.join(direccion, 'train.txt'), kb_index)\n",
        "    inplace_shuffle(*train_data)\n",
        "    valid_data = read_data(os.path.join(direccion, 'valid.txt'), kb_index)\n",
        "    test_data = read_data(os.path.join(direccion, 'test.txt'), kb_index)\n",
        "    filt_heads, filt_tails = heads_tails(n_ent, train_data, valid_data, test_data)\n",
        "    valid_data = [torch.LongTensor(vec) for vec in valid_data]\n",
        "    test_data = [torch.LongTensor(vec) for vec in test_data]\n",
        "    tester = lambda: dis.test_link(valid_data, n_ent, filt_heads, filt_tails)\n",
        "    train_data = [torch.LongTensor(vec) for vec in train_data]\n",
        "\n",
        "    mrr_result=dis.test_link(test_data, n_ent, filt_heads, filt_tails)\n",
        "    print('El MRR del modelo discriminador antes de ser entrenado con GAN para test_data es de :',mrr_result)\n",
        "\n",
        "    corrupter = BernCorrupterMulti(train_data, n_ent, n_rel, config().adv.n_sample)\n",
        "    src, rel, dst = train_data\n",
        "    n_train = len(src)\n",
        "    n_epoch = config().adv.n_epoch\n",
        "    n_batch = config().adv.n_batch\n",
        "    mdl_name = 'gan_dis_' + datetime.datetime.now().strftime(\"%m%d%H%M%S\") + '.mdl'\n",
        "    best_perf = 0\n",
        "    avg_reward = 0\n",
        "    for epoch in range(n_epoch):\n",
        "        epoch_d_loss = 0\n",
        "        epoch_reward = 0\n",
        "        src_cand, rel_cand, dst_cand = corrupter.corrupt(src, rel, dst, keep_truth=False)\n",
        "        for _s, _r, _t, ss, rs, ts in batch_by_num(n_batch, src, rel, dst, src_cand, rel_cand, dst_cand, n_sample=n_train):\n",
        "            gen_step = gen.gen_step(ss, rs, ts, temperature=config().adv.temperature)\n",
        "            src_smpl, dst_smpl = next(gen_step)\n",
        "            pdb.set_trace()\n",
        "            losses, rewards = dis.dis_step(_s, _r, _t, src_smpl.squeeze(), dst_smpl.squeeze())\n",
        "            epoch_reward += torch.sum(rewards)\n",
        "            rewards = rewards - avg_reward\n",
        "            gen_step.send(rewards.unsqueeze(1))\n",
        "            epoch_d_loss += torch.sum(losses)\n",
        "            #print('epoch_loss', epoch_d_loss.item())\n",
        "            #print('recompensa', torch.sum(rewards).item())\n",
        "        avg_loss = epoch_d_loss / n_train\n",
        "        avg_reward = epoch_reward / n_train\n",
        "        logging.info('Epoch %d/%d, D_loss=%f, reward=%f', epoch + 1, n_epoch, avg_loss, avg_reward)\n",
        "        if (epoch + 1) % config().adv.epoch_per_test == 0:\n",
        "            #gen.test_link(valid_data, n_ent, filt_heads, filt_tails)\n",
        "            perf = dis.test_link(valid_data, n_ent, filt_heads, filt_tails)\n",
        "            print('El MRR en epoch ',epoch,'para valid_data fue de :', perf)\n",
        "            if perf > best_perf:\n",
        "                best_perf = perf\n",
        "                task_dir = config().task.dir\n",
        "                direccion=os.path.join(data_dir, task_dir)\n",
        "                dis.save(os.path.join(direccion, mdl_name))\n",
        "    dis.load(os.path.join(config().task.dir, mdl_name))\n",
        "    mrr_result=dis.test_link(test_data, n_ent, filt_heads, filt_tails)\n",
        "    print('El MRR del modelo discriminador despues de ser entrenado con GAN para test_data es de :',mrr_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dazvUX91UxoQ",
        "outputId": "bc837428-6081-487d-ce1e-d383b78d3d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-8ae3e96e2c45>:90: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  all_var = Variable(torch.arange(0, n_ent).unsqueeze(0).expand(batch_size, n_ent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La suma de los mrr es:  tensor(7019.7583, device='cuda:0') la cantidad de mrr calculados en el test es de: 40932\n",
            "El MRR del modelo discriminador antes de ser entrenado con GAN para test_data es de : tensor(0.1715, device='cuda:0')\n",
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(38)\u001b[0;36mgen_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     36 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mgen_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     37 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 38 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'opt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     39 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     40 \u001b[0;31m        \u001b[0m_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-8ae3e96e2c45>:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs=f.softmax(logits)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m<ipython-input-44-0385cfc56ee7>\u001b[0m(47)\u001b[0;36mgan_train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     45 \u001b[0;31m            \u001b[0msrc_smpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_smpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     46 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 47 \u001b[0;31m            \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdis_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_smpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_smpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     48 \u001b[0;31m            \u001b[0mepoch_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     49 \u001b[0;31m            \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(63)\u001b[0;36mdis_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     61 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mdis_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     62 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 63 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'opt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     64 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     65 \u001b[0;31m        \u001b[0msrc_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n",
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(52)\u001b[0;36mgen_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     50 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0msample_srcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_dsts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     51 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 52 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     53 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     54 \u001b[0;31m            \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> rewards \n",
            "tensor([[ -9.4031],\n",
            "        [ -6.2058],\n",
            "        [ -8.9969],\n",
            "        ...,\n",
            "        [-10.6759],\n",
            "        [ -5.5783],\n",
            "        [ -6.4716]], device='cuda:0')\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(53)\u001b[0;36mgen_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     51 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     52 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 53 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     54 \u001b[0;31m            \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     55 \u001b[0;31m            \u001b[0mreinforce_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(54)\u001b[0;36mgen_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     52 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     53 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 54 \u001b[0;31m            \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     55 \u001b[0;31m            \u001b[0mreinforce_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     56 \u001b[0;31m            \u001b[0mreinforce_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> logits\n",
            "tensor([[-1.1474e+00, -2.0135e-01,  2.3318e-02,  ...,  3.3007e-01,\n",
            "          7.4245e-01, -4.2378e-01],\n",
            "        [-1.6151e+00, -4.6789e-01,  3.0509e+00,  ..., -2.7377e+00,\n",
            "         -3.2800e+00,  1.5270e+00],\n",
            "        [ 3.6110e-01,  2.8861e-01, -1.7388e-04,  ...,  8.6727e-02,\n",
            "         -4.9752e-01, -3.3915e-01],\n",
            "        ...,\n",
            "        [-7.8744e-01,  2.5421e-01,  1.4100e+00,  ...,  7.0408e-01,\n",
            "          3.5174e-01, -4.7534e-01],\n",
            "        [ 8.6035e-01, -1.4700e+00,  1.1259e+00,  ...,  6.4508e-01,\n",
            "         -1.0397e+00,  7.0513e+00],\n",
            "        [-1.7097e+00,  7.1311e-01, -5.0418e-01,  ...,  4.8507e+00,\n",
            "          1.0525e+00,  4.2451e-01]], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "ipdb> probs\n",
            "tensor([[1.5241e-02, 3.9253e-02, 4.9141e-02,  ..., 6.6783e-02, 1.0087e-01,\n",
            "         3.1425e-02],\n",
            "        [5.4633e-04, 1.7206e-03, 5.8059e-02,  ..., 1.7779e-04, 1.0337e-04,\n",
            "         1.2650e-02],\n",
            "        [6.6342e-02, 6.1703e-02, 4.6227e-02,  ..., 5.0423e-02, 2.8112e-02,\n",
            "         3.2936e-02],\n",
            "        ...,\n",
            "        [2.1226e-02, 6.0153e-02, 1.9107e-01,  ..., 9.4326e-02, 6.6315e-02,\n",
            "         2.9001e-02],\n",
            "        [1.3743e-03, 1.3366e-04, 1.7922e-03,  ..., 1.1081e-03, 2.0554e-04,\n",
            "         6.7112e-01],\n",
            "        [9.0161e-04, 1.0168e-02, 3.0100e-03,  ..., 6.3704e-01, 1.4276e-02,\n",
            "         7.6189e-03]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "ipdb> f.log_softmax(logits)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<stdin>:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-4.1838, -3.2377, -3.0131,  ..., -2.7063, -2.2939, -3.4602],\n",
            "        [-7.5123, -6.3651, -2.8463,  ..., -8.6349, -9.1772, -4.3701],\n",
            "        [-2.7129, -2.7854, -3.0742,  ..., -2.9873, -3.5715, -3.4132],\n",
            "        ...,\n",
            "        [-3.8525, -2.8109, -1.6551,  ..., -2.3610, -2.7133, -3.5404],\n",
            "        [-6.5898, -8.9202, -6.3243,  ..., -6.8051, -8.4899, -0.3988],\n",
            "        [-7.0113, -4.5885, -5.8058,  ..., -0.4509, -4.2492, -4.8771]],\n",
            "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
            "ipdb> n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-8ae3e96e2c45>:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  log_probs = f.log_softmax(logits)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(55)\u001b[0;36mgen_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     53 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     54 \u001b[0;31m            \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 55 \u001b[0;31m            \u001b[0mreinforce_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     56 \u001b[0;31m            \u001b[0mreinforce_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     57 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> log_probs \n",
            "tensor([[-4.1838, -3.2377, -3.0131,  ..., -2.7063, -2.2939, -3.4602],\n",
            "        [-7.5123, -6.3651, -2.8463,  ..., -8.6349, -9.1772, -4.3701],\n",
            "        [-2.7129, -2.7854, -3.0742,  ..., -2.9873, -3.5715, -3.4132],\n",
            "        ...,\n",
            "        [-3.8525, -2.8109, -1.6551,  ..., -2.3610, -2.7133, -3.5404],\n",
            "        [-6.5898, -8.9202, -6.3243,  ..., -6.8051, -8.4899, -0.3988],\n",
            "        [-7.0113, -4.5885, -5.8058,  ..., -0.4509, -4.2492, -4.8771]],\n",
            "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
            "ipdb> rewards\n",
            "tensor([[ -9.4031],\n",
            "        [ -6.2058],\n",
            "        [ -8.9969],\n",
            "        ...,\n",
            "        [-10.6759],\n",
            "        [ -5.5783],\n",
            "        [ -6.4716]], device='cuda:0')\n",
            "ipdb> row_idx.cuda()\n",
            "tensor([[   0],\n",
            "        [   1],\n",
            "        [   2],\n",
            "        ...,\n",
            "        [2718],\n",
            "        [2719],\n",
            "        [2720]], device='cuda:0')\n",
            "ipdb> sample_idx.data]\n",
            "*** SyntaxError: unmatched ']'\n",
            "ipdb> sample_idx.data\n",
            "tensor([[10],\n",
            "        [ 9],\n",
            "        [ 4],\n",
            "        ...,\n",
            "        [17],\n",
            "        [19],\n",
            "        [13]], device='cuda:0')\n",
            "ipdb> log_probs\n",
            "tensor([[-4.1838, -3.2377, -3.0131,  ..., -2.7063, -2.2939, -3.4602],\n",
            "        [-7.5123, -6.3651, -2.8463,  ..., -8.6349, -9.1772, -4.3701],\n",
            "        [-2.7129, -2.7854, -3.0742,  ..., -2.9873, -3.5715, -3.4132],\n",
            "        ...,\n",
            "        [-3.8525, -2.8109, -1.6551,  ..., -2.3610, -2.7133, -3.5404],\n",
            "        [-6.5898, -8.9202, -6.3243,  ..., -6.8051, -8.4899, -0.3988],\n",
            "        [-7.0113, -4.5885, -5.8058,  ..., -0.4509, -4.2492, -4.8771]],\n",
            "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
            "ipdb> log_probs.shape\n",
            "torch.Size([2721, 20])\n",
            "ipdb> rewards.shape\n",
            "torch.Size([2721, 1])\n",
            "ipdb> log_probs[row_idx.cuda(), sample_idx.data]\n",
            "tensor([[-2.6566],\n",
            "        [-1.7234],\n",
            "        [-3.6130],\n",
            "        ...,\n",
            "        [-2.3610],\n",
            "        [-0.3988],\n",
            "        [-3.0475]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(56)\u001b[0;36mgen_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     54 \u001b[0;31m            \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     55 \u001b[0;31m            \u001b[0mreinforce_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 56 \u001b[0;31m            \u001b[0mreinforce_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     57 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     58 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> reinforce_loss \n",
            "tensor(-44682.7773, device='cuda:0', grad_fn=<NegBackward0>)\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(57)\u001b[0;36mgen_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     55 \u001b[0;31m            \u001b[0mreinforce_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     56 \u001b[0;31m            \u001b[0mreinforce_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 57 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     58 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     59 \u001b[0;31m        \u001b[0;32myield\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(58)\u001b[0;36mgen_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     56 \u001b[0;31m            \u001b[0mreinforce_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     57 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 58 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     59 \u001b[0;31m        \u001b[0;32myield\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     60 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(59)\u001b[0;36mgen_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     57 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     58 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 59 \u001b[0;31m        \u001b[0;32myield\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     60 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     61 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mdis_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(38)\u001b[0;36mgen_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     36 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mgen_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     37 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 38 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'opt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     39 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     40 \u001b[0;31m        \u001b[0m_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> n\n",
            "> \u001b[0;32m<ipython-input-33-8ae3e96e2c45>\u001b[0m(40)\u001b[0;36mgen_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     38 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'opt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     39 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 40 \u001b[0;31m        \u001b[0m_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     41 \u001b[0;31m        \u001b[0mrel_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     42 \u001b[0;31m        \u001b[0msrc_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> q\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    #pretrain()\n",
        "    gan_train()\n",
        "    print('Fin')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP3fIKlXrb+PIzpa8coEWC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}