{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yalopez84/GAN_study/blob/master/Estudiando_KBGAN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KTch7ZJ4SHYH"
      },
      "outputs": [],
      "source": [
        "#Libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "from torch.optim import Adam, SGD, Adagrad\n",
        "from torch.autograd import Variable\n",
        "from random import randint\n",
        "from collections import defaultdict\n",
        "from numpy.random import choice, randint\n",
        "import numpy as n\n",
        "import datetime\n",
        "import yaml\n",
        "import sys\n",
        "import logging\n",
        "import subprocess\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU5XnYitQYvg",
        "outputId": "867b641d-fe18-457d-9bc8-be39ab7ad979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device cuda\n"
          ]
        }
      ],
      "source": [
        "#Directories and devices\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir=\"/content/drive/MyDrive/NegativeStrategies/OAGAN-NS/data/\"\n",
        "os.chdir(data_dir)\n",
        "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "print(\"Device\",device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iok96m3hTzko"
      },
      "outputs": [],
      "source": [
        "#base_model.py\n",
        "class BaseModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseModule,self).__init__()\n",
        "    def score(self,src,rel,dst):\n",
        "        raise NotImplementedError\n",
        "    def dist(self,src,rel,dst):\n",
        "        raise NotImplementedError\n",
        "    def prob_logit(self,src,rel,dst):\n",
        "        raise NotImplementedError\n",
        "    def prob(self,src,rel,dst):\n",
        "        return f.softmax(self.prob_logit(src,rel,dst))\n",
        "    def constraint(self):\n",
        "        pass\n",
        "    def pair_loss(self,src,rel,dst, src_bad,dst_bad):\n",
        "        d_good=self.dist(src,rel,dst)\n",
        "        d_bad=self.dist(src_bad,rel,dst_bad)\n",
        "        return f.relu(self.margin + d_good - d_bad)\n",
        "\n",
        "    def softmax_loss(self, src, rel, dst, truth):\n",
        "        probs=self.prob(src,rel,dst)\n",
        "        n=probs.size(0)\n",
        "        truth_probs=torch.log(probs[torch.arange(0,n).type(torch.LongTensor).cuda(),truth]+1e-30)\n",
        "        return -truth_probs\n",
        "\n",
        "class BaseModel(object):\n",
        "    def __init__(self):\n",
        "        self.mdl= None\n",
        "        self.weight_decay = 0\n",
        "    def save(self,filename):\n",
        "        torch.save(self.mdl.state_dict(),filename)\n",
        "\n",
        "    def load(self,filename):\n",
        "        self.mdl.load_state_dict(torch.load(filename,map_location=lambda storage, location:storage.cuda()))\n",
        "\n",
        "    def gen_step(self,src,rel,dst,n_sample=1,temperature=1.0,train=True):\n",
        "        pdb.set_trace()\n",
        "        if not hasattr(self,'opt'):\n",
        "            self.opt=Adam(self.mdl.parameters(), weight_decay=self.weight_decay)\n",
        "        _n,_m=dst.size()\n",
        "        rel_var=Variable(rel.cuda())\n",
        "        src_var = Variable(src.cuda())\n",
        "        dst_var = Variable(dst.cuda())\n",
        "        logits=self.mdl.prob_logit(src_var, rel_var, dst_var)/temperature\n",
        "        probs=f.softmax(logits)\n",
        "        row_idx=torch.arange(0,_n).type(torch.LongTensor).unsqueeze(1).expand(_n, n_sample)\n",
        "        sample_idx=torch.multinomial(probs,n_sample, replacement=True)\n",
        "        sample_srcs = src[row_idx, sample_idx.data.cpu()]\n",
        "        sample_dsts = dst[row_idx, sample_idx.data.cpu()]\n",
        "        rewards = yield sample_srcs, sample_dsts\n",
        "        pdb.set_trace()\n",
        "        if train:\n",
        "            self.mdl.zero_grad()\n",
        "            log_probs = f.log_softmax(logits)\n",
        "            reinforce_loss = -torch.sum(Variable(rewards) * log_probs[row_idx.cuda(), sample_idx.data])\n",
        "            reinforce_loss.backward()\n",
        "            self.opt.step()\n",
        "            self.mdl.constraint()\n",
        "        yield None\n",
        "\n",
        "    def dis_step(self,src, rel, dst, src_fake, dst_fake, train=True):\n",
        "        pdb.set_trace()\n",
        "        if not hasattr(self,'opt'):\n",
        "            self.opt = Adam(self.mdl.parameters(), weight_decay=self.weight_decay)\n",
        "        src_var = Variable(src.cuda())\n",
        "        rel_var = Variable(rel.cuda())\n",
        "        dst_var = Variable(dst.cuda())\n",
        "        src_fake_var = Variable(src_fake.cuda())\n",
        "        dst_fake_var = Variable(dst_fake.cuda())\n",
        "        losses = self.mdl.pair_loss(src_var, rel_var, dst_var, src_fake_var, dst_fake_var)\n",
        "        fake_scores = self.mdl.score(src_fake_var, rel_var, dst_fake_var)\n",
        "        if train:\n",
        "            self.mdl.zero_grad()\n",
        "            torch.sum(losses).backward()\n",
        "            self.opt.step()\n",
        "            self.mdl.constraint()\n",
        "        return losses.data, -fake_scores.data\n",
        "\n",
        "    def test_link(self, test_data, n_ent, heads, tails, filt=False):\n",
        "\n",
        "        mrr_tot = 0\n",
        "        mr_tot = 0\n",
        "        hit10_tot = 0\n",
        "        count = 0\n",
        "        for batch_s, batch_r, batch_t in batch_by_size(config().test_batch_size, *test_data):\n",
        "            batch_size = batch_s.size(0)\n",
        "            rel_var = Variable(batch_r.unsqueeze(1).expand(batch_size, n_ent).cuda())\n",
        "            src_var = Variable(batch_s.unsqueeze(1).expand(batch_size, n_ent).cuda())\n",
        "            dst_var = Variable(batch_t.unsqueeze(1).expand(batch_size, n_ent).cuda())\n",
        "            all_var = Variable(torch.arange(0, n_ent).unsqueeze(0).expand(batch_size, n_ent)\n",
        "                               .type(torch.LongTensor).cuda(), volatile=True)\n",
        "            with torch.no_grad():\n",
        "                batch_dst_scores = self.mdl.score(src_var, rel_var, all_var).data\n",
        "                batch_src_scores = self.mdl.score(all_var, rel_var, dst_var).data\n",
        "\n",
        "            for ss, rr, tt, dst_scores, src_scores in zip(batch_s, batch_r, batch_t, batch_dst_scores, batch_src_scores):\n",
        "\n",
        "                if filt:\n",
        "                    if tails[(ss.item(), rr.item())]._nnz() > 1:\n",
        "                        tmp = dst_scores[tt]\n",
        "                        dst_scores += tails[(ss.item(), rr.item())].cuda() * 1e30\n",
        "                        dst_scores[tt] = tmp\n",
        "\n",
        "                    if heads[(tt.item(), rr.item())]._nnz() > 1:\n",
        "                        tmp = src_scores[ss]\n",
        "                        src_scores += heads[(tt.item(), rr.item())].cuda() * 1e30\n",
        "                        src_scores[ss] = tmp\n",
        "                mrr, mr, hit10 =mrr_mr_hitk(dst_scores, tt)\n",
        "                mrr_tot += mrr\n",
        "                mr_tot += mr\n",
        "                hit10_tot += hit10\n",
        "                mrr, mr, hit10 = mrr_mr_hitk(src_scores, ss)\n",
        "                mrr_tot += mrr\n",
        "                mr_tot += mr\n",
        "                hit10_tot += hit10\n",
        "                count += 2\n",
        "        logging.info('Test_MRR=%f, Test_MR=%f, Test_H@10=%f', mrr_tot / count, mr_tot / count, hit10_tot / count)\n",
        "        print('La suma de los mrr es: ', mrr_tot, 'la cantidad de mrr calculados en el test es de:', count)\n",
        "        return mrr_tot / count\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "U9-d4Nx7RmKR"
      },
      "outputs": [],
      "source": [
        "#data_utils.py\n",
        "def heads_tails(n_ent, train_data, valid_data=None, test_data=None):\n",
        "    train_src, train_rel, train_dst = train_data\n",
        "    if valid_data:\n",
        "        valid_src, valid_rel, valid_dst = valid_data\n",
        "    else:\n",
        "        valid_src = valid_rel = valid_dst = []\n",
        "    if test_data:\n",
        "        test_src, test_rel, test_dst = test_data\n",
        "    else:\n",
        "        test_src = test_rel = test_dst = []\n",
        "    all_src = train_src + valid_src + test_src\n",
        "    all_rel = train_rel + valid_rel + test_rel\n",
        "    all_dst = train_dst + valid_dst + test_dst\n",
        "    heads = defaultdict(lambda: set())\n",
        "    tails = defaultdict(lambda: set())\n",
        "    for ss, rr, tt in zip(all_src, all_rel, all_dst):\n",
        "        tails[(ss, rr)].add(tt)\n",
        "        heads[(tt, rr)].add(ss)\n",
        "    heads_sp = {}\n",
        "    tails_sp = {}\n",
        "    for k in tails.keys():\n",
        "        tails_sp[k] = torch.sparse.FloatTensor(torch.LongTensor([list(tails[k])]),\n",
        "                                               torch.ones(len(tails[k])), torch.Size([n_ent]))\n",
        "    for k in heads.keys():\n",
        "        heads_sp[k] = torch.sparse.FloatTensor(torch.LongTensor([list(heads[k])]),\n",
        "                                               torch.ones(len(heads[k])), torch.Size([n_ent]))\n",
        "    return heads_sp, tails_sp\n",
        "\n",
        "\n",
        "def inplace_shuffle(*lists):\n",
        "    idx = []\n",
        "    for i in range(len(lists[0])):\n",
        "        a=i\n",
        "        if a==0:\n",
        "            a=1\n",
        "        idx.append(randint(0, a))\n",
        "    for ls in lists:\n",
        "        for i, item in enumerate(ls):\n",
        "            j = idx[i]\n",
        "            ls[i], ls[j] = ls[j], ls[i]\n",
        "\n",
        "\n",
        "def batch_by_num(n_batch, *lists, n_sample=None):\n",
        "    if n_sample is None:\n",
        "        n_sample = len(lists[0])\n",
        "    for i in range(n_batch):\n",
        "        head = int(n_sample * i / n_batch)\n",
        "        tail = int(n_sample * (i + 1) / n_batch)\n",
        "        ret = [ls[head:tail] for ls in lists]\n",
        "        if len(ret) > 1:\n",
        "            yield ret\n",
        "        else:\n",
        "            yield ret[0]\n",
        "\n",
        "def batch_by_size(batch_size, *lists, n_sample=None):\n",
        "    if n_sample is None:\n",
        "        n_sample = len(lists[0])\n",
        "    head = 0\n",
        "    while head < n_sample:\n",
        "        tail = min(n_sample, head + batch_size)\n",
        "        ret = [ls[head:tail] for ls in lists]\n",
        "        head += batch_size\n",
        "        if len(ret) > 1:\n",
        "            yield ret\n",
        "        else:\n",
        "            yield ret[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BKH-NlZYS9ox"
      },
      "outputs": [],
      "source": [
        "#config.py\n",
        "class ConfigDict(dict):\n",
        "    __getattr__ = dict.__getitem__\n",
        "\n",
        "def _make_config_dict(obj):\n",
        "\n",
        "    if isinstance(obj, dict):\n",
        "        return ConfigDict({k: _make_config_dict(v) for k, v in obj.items()})\n",
        "    elif isinstance(obj, list):\n",
        "        return [_make_config_dict(x) for x in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "_config = None\n",
        "\n",
        "def config():\n",
        "    arg_dict ={\n",
        "        '--config':'config_fb15k237.yaml',\n",
        "        '--pretrain_config':'<model_name>'\n",
        "    }\n",
        "    global _config\n",
        "    if _config is None:\n",
        "        config_path='config_fb15k237.yaml'\n",
        "        with open(os.path.join(data_dir, config_path)) as f:\n",
        "            _config = _make_config_dict(yaml.full_load(f))\n",
        "    return _config\n",
        "\n",
        "def _dump_config(obj, prefix):\n",
        "    if isinstance(obj, dict):\n",
        "        for k, v in obj.items():\n",
        "            _dump_config(v, prefix + (k,))\n",
        "    elif isinstance(obj, list):\n",
        "        for i, v in enumerate(obj):\n",
        "            _dump_config(v, prefix + (str(i),))\n",
        "    else:\n",
        "        if isinstance(obj, str):\n",
        "            rep = obj\n",
        "        else:\n",
        "            rep = repr(obj)\n",
        "        logging.debug('%s=%s', '.'.join(prefix), rep)\n",
        "\n",
        "def dump_config():\n",
        "    return _dump_config(_config, tuple())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Mnasg894VZJs"
      },
      "outputs": [],
      "source": [
        "#metrics.py\n",
        "def mrr_mr_hitk(scores, target, k=10):\n",
        "    values, sorted_idx = torch.sort(scores)\n",
        "    find_target = sorted_idx == target\n",
        "    target_rank = torch.nonzero(find_target)[0, 0] + 1\n",
        "    return 1 / target_rank, target_rank, int(target_rank <= k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6vTg8ojpcZkT"
      },
      "outputs": [],
      "source": [
        "#corrupter.py\n",
        "def get_bern_prob(data, n_ent, n_rel):\n",
        "    src, rel, dst = data\n",
        "    edges = defaultdict(lambda: defaultdict(lambda: set()))\n",
        "    rev_edges = defaultdict(lambda: defaultdict(lambda: set()))\n",
        "    for ss, rr, tt in zip(src, rel, dst):\n",
        "        edges[rr.item()][ss.item()].add(tt.item())\n",
        "        rev_edges[rr.item()][tt.item()].add(ss.item())\n",
        "    bern_prob = torch.zeros(n_rel)\n",
        "    for rrr in edges.keys():\n",
        "        tph = sum(len(tails) for tails in edges[rrr].values()) / len(edges[rrr])\n",
        "        htp = sum(len(heads) for heads in rev_edges[rrr].values()) / len(rev_edges[rrr])\n",
        "        bern_prob[rrr] = tph / (tph + htp)\n",
        "    return bern_prob\n",
        "\n",
        "class BernCorrupter(object):\n",
        "    def __init__(self, data, n_ent, n_rel):\n",
        "        self.bern_prob = get_bern_prob(data, n_ent, n_rel)\n",
        "        self.n_ent = n_ent\n",
        "\n",
        "    def corrupt(self, src, rel, dst):\n",
        "        prob = self.bern_prob[rel]\n",
        "        selection = torch.bernoulli(prob).numpy().astype('int64')\n",
        "        ent_random = choice(self.n_ent, len(src))\n",
        "        src_out = (1 - selection) * src.numpy() + selection * ent_random\n",
        "        dst_out = selection * dst.numpy() + (1 - selection) * ent_random\n",
        "        return torch.from_numpy(src_out), torch.from_numpy(dst_out)\n",
        "\n",
        "class BernCorrupterMulti(object):\n",
        "    def __init__(self, data, n_ent, n_rel, n_sample):\n",
        "        self.bern_prob = get_bern_prob(data, n_ent, n_rel)\n",
        "        self.n_ent = n_ent\n",
        "        self.n_sample = n_sample\n",
        "\n",
        "    def corrupt(self, src, rel, dst, keep_truth=True):\n",
        "        nn = len(src)\n",
        "        prob = self.bern_prob[rel]\n",
        "        selection = torch.bernoulli(prob).numpy().astype('bool')\n",
        "        src_out = n.tile(src.numpy(), (self.n_sample, 1)).transpose()\n",
        "        dst_out = n.tile(dst.numpy(), (self.n_sample, 1)).transpose()\n",
        "        rel_out = rel.unsqueeze(1).expand(nn, self.n_sample)\n",
        "        if keep_truth:\n",
        "            ent_random = choice(self.n_ent, (nn, self.n_sample - 1))\n",
        "            src_out[selection, 1:] = ent_random[selection]\n",
        "            dst_out[~selection, 1:] = ent_random[~selection]\n",
        "        else:\n",
        "            ent_random = choice(self.n_ent, (nn, self.n_sample))\n",
        "            src_out[selection, :] = ent_random[selection]\n",
        "            dst_out[~selection, :] = ent_random[~selection]\n",
        "        return torch.from_numpy(src_out), rel_out, torch.from_numpy(dst_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OgZcjk8DLwYo"
      },
      "outputs": [],
      "source": [
        "#read_data.py\n",
        "KBIndex = namedtuple('KBIndex', ['ent_list', 'rel_list', 'ent_id', 'rel_id'])\n",
        "\n",
        "def index_ent_rel(*filenames):\n",
        "    ent_set = set()\n",
        "    rel_set = set()\n",
        "    for filename in filenames:\n",
        "        with open(filename) as f:\n",
        "            for ln in f:\n",
        "                s, r, t = ln.strip().split('\\t')[:3]\n",
        "                ent_set.add(s)\n",
        "                ent_set.add(t)\n",
        "                rel_set.add(r)\n",
        "    ent_list = sorted(list(ent_set))\n",
        "    rel_list = sorted(list(rel_set))\n",
        "    ent_id = dict(zip(ent_list, count()))\n",
        "    rel_id = dict(zip(rel_list, count()))\n",
        "    return KBIndex(ent_list, rel_list, ent_id, rel_id)\n",
        "\n",
        "\n",
        "def graph_size(kb_index):\n",
        "    return len(kb_index.ent_id), len(kb_index.rel_id)\n",
        "\n",
        "\n",
        "def read_data(filename, kb_index):\n",
        "    src = []\n",
        "    rel = []\n",
        "    dst = []\n",
        "    with open(filename) as f:\n",
        "        for ln in f:\n",
        "            s, r, t = ln.strip().split('\\t')\n",
        "            src.append(kb_index.ent_id[s])\n",
        "            rel.append(kb_index.rel_id[r])\n",
        "            dst.append(kb_index.ent_id[t])\n",
        "    return src, rel, dst"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tripletas_by_texto():\n",
        "\n",
        "    task_dir = config().task.dir\n",
        "    direccion=os.path.join(data_dir, task_dir)\n",
        "    kb_index = index_ent_rel(os.path.join(direccion, 'train.txt'),\n",
        "                             os.path.join(direccion, 'valid.txt'),\n",
        "                             os.path.join(direccion, 'test.txt'))\n",
        "\n",
        "    # Tripletas de ejemplo en forma de cadenas\n",
        "    src=['/m/0125ny', '/m/011hq1', '/m/0102t4','/m/0np9r']\n",
        "    rel=['/soccer/football_team/current_roster./sports/sports_team_roster/position','/film/film/story_by','/education/university/international_tuition./measurement_unit/dated_money_value/currency','/film/film/dubbing_performances./film/dubbing_performance/actor']\n",
        "    dst=['/m/01hbq0','/m/013rds','/m/0100mt','/m/0127m7']\n",
        "\n",
        "    #SUJETO\n",
        "    src_result=[]\n",
        "    elementos_ent=kb_index.ent_id.items()\n",
        "    pos_src=0\n",
        "    for cadena in src:\n",
        "        pos_src=0\n",
        "        for k,v in elementos_ent:\n",
        "            if k==cadena:\n",
        "                src_result.append(v)\n",
        "                break\n",
        "            pos_src+=1\n",
        "        if pos_src== len(elementos_ent):\n",
        "            src_result.append(-1)\n",
        "    src_result\n",
        "\n",
        "    #PREDICADO\n",
        "    rel_result=[]\n",
        "    elementos_rel=kb_index.rel_id.items()\n",
        "    pos_rel=0\n",
        "    for cadena in rel:\n",
        "        pos_rel=0\n",
        "        for k,v in elementos_rel:\n",
        "            if k==cadena:\n",
        "                rel_result.append(v)\n",
        "                break\n",
        "            pos_rel+=1\n",
        "        if pos_rel== len(elementos_rel):\n",
        "            rel_result.append(-1)\n",
        "    rel_result\n",
        "\n",
        "    #OBJETO\n",
        "    dst_result=[]\n",
        "    elementos_ent=kb_index.ent_id.items()\n",
        "    pos_dst=0\n",
        "    for cadena in dst:\n",
        "        pos_dst=0\n",
        "        for k,v in elementos_ent:\n",
        "            if k==cadena:\n",
        "                dst_result.append(v)\n",
        "                break\n",
        "            pos_dst+=1\n",
        "        if pos_dst== len(elementos_ent):\n",
        "            dst_result.append(-1)\n",
        "    dst_result\n",
        "\n",
        "    #Se crean como tripletas y se imprimen\n",
        "    class Tripleta(object):\n",
        "        def __init__(self, sujeto, predicado, objeto):\n",
        "            self.sujeto=sujeto\n",
        "            self.predicado=predicado\n",
        "            self.objeto=objeto\n",
        "\n",
        "    tripletas =[]\n",
        "    for s,r,t in zip(src_result, rel_result,dst_result):\n",
        "        tripletas.append(Tripleta(s,r,t))\n",
        "\n",
        "    for i in tripletas:\n",
        "        print(str(i.sujeto) + '\\t' + str(i.predicado) + '\\t'+ str(i.objeto))\n",
        "\n",
        "def tripletas_by_identificadores():\n",
        "\n",
        "    task_dir = config().task.dir\n",
        "    direccion=os.path.join(data_dir, task_dir)\n",
        "    kb_index = index_ent_rel(os.path.join(direccion, 'train.txt'),\n",
        "                             os.path.join(direccion, 'valid.txt'),\n",
        "                             os.path.join(direccion, 'test.txt'))\n",
        "\n",
        "    # Tripletas de ejemplo en forma de identificadores\n",
        "    src=[100, 32, 2,13890]\n",
        "    rel=[200,98,65,76]\n",
        "    dst=[1677,290,1,111]\n",
        "\n",
        "    #SUJETO\n",
        "    src_result=[]\n",
        "    elementos_ent=kb_index.ent_id.items()\n",
        "    pos_src=0\n",
        "    for indice in src:\n",
        "        pos_src=0\n",
        "        for k,v in elementos_ent:\n",
        "            if v==indice:\n",
        "                src_result.append(k)\n",
        "                break\n",
        "            pos_src+=1\n",
        "        if pos_src== len(elementos_ent):\n",
        "            src_result.append('No aparece')\n",
        "\n",
        "    #PREDICADO\n",
        "    rel_result=[]\n",
        "    elementos_rel=kb_index.rel_id.items()\n",
        "    pos_rel=0\n",
        "    for indice in rel:\n",
        "        pos_rel=0\n",
        "        for k,v in elementos_rel:\n",
        "            if v==indice:\n",
        "                rel_result.append(k)\n",
        "                break\n",
        "            pos_rel+=1\n",
        "        if pos_rel== len(elementos_rel):\n",
        "            rel_result.append('No aparece')\n",
        "\n",
        "    #OBJETO\n",
        "    dst_result=[]\n",
        "    elementos_ent=kb_index.ent_id.items()\n",
        "    pos_dst=0\n",
        "    for indice in dst:\n",
        "        pos_dst=0\n",
        "        for k,v in elementos_ent:\n",
        "            if v==indice:\n",
        "                dst_result.append(k)\n",
        "                break\n",
        "            pos_dst+=1\n",
        "        if pos_dst== len(elementos_ent):\n",
        "            dst_result.append('No aparece')\n",
        "\n",
        "    #Se crean como tripletas y se imprimen\n",
        "    class Tripleta(object):\n",
        "        def __init__(self, sujeto, predicado, objeto):\n",
        "            self.sujeto=sujeto\n",
        "            self.predicado=predicado\n",
        "            self.objeto=objeto\n",
        "\n",
        "    tripletas =[]\n",
        "    for s,r,t in zip(src_result, rel_result,dst_result):\n",
        "        tripletas.append(Tripleta(s,r,t))\n",
        "\n",
        "    for i in tripletas:\n",
        "        print(i.sujeto + '\\t' + i.predicado + '\\t'+ i.objeto)"
      ],
      "metadata": {
        "id": "kJdZ8PdXTUpK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CRDCvDPdIKWe"
      },
      "outputs": [],
      "source": [
        "#trans_e.py\n",
        "class TransEModule(BaseModule):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(TransEModule, self).__init__()\n",
        "        self.p = config.p\n",
        "        self.margin = config.margin\n",
        "        self.temp = config.get('temp', 1)\n",
        "        self.rel_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.ent_embed = nn.Embedding(n_ent, config.dim)\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.normal_(1 / param.size(1) ** 0.5)\n",
        "            param.data.renorm_(2, 0, 1)\n",
        "\n",
        "    def forward(self, src, rel, dst):\n",
        "        return torch.norm(self.ent_embed(dst) - self.ent_embed(src) - self.rel_embed(rel) + 1e-30, p=self.p, dim=-1)\n",
        "\n",
        "    def dist(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "    def score(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "    def prob_logit(self, src, rel, dst):\n",
        "        return -self.forward(src, rel ,dst) / self.temp\n",
        "\n",
        "    def constraint(self):\n",
        "        self.ent_embed.weight.data.renorm_(2, 0, 1)\n",
        "        self.rel_embed.weight.data.renorm_(2, 0, 1)\n",
        "\n",
        "class TransE(BaseModel):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(TransE, self).__init__()\n",
        "        self.mdl = TransEModule(n_ent, n_rel, config)\n",
        "        self.mdl.cuda()\n",
        "        self.config = config\n",
        "\n",
        "    def pretrain(self, train_data, corrupter, tester):\n",
        "\n",
        "        src, rel, dst = train_data\n",
        "        n_train = len(src)\n",
        "        optimizer = Adam(self.mdl.parameters())\n",
        "        #optimizer = SGD(self.mdl.parameters(), lr=1e-4)\n",
        "        n_epoch = self.config.n_epoch\n",
        "        n_batch = self.config.n_batch\n",
        "        best_perf = 0\n",
        "        for epoch in range(n_epoch):\n",
        "            print('---------epoch_number---', epoch)\n",
        "            epoch_loss = 0\n",
        "            rand_idx = torch.randperm(n_train)\n",
        "            src = src[rand_idx]\n",
        "            rel = rel[rand_idx]\n",
        "            dst = dst[rand_idx]\n",
        "            src_corrupted, dst_corrupted = corrupter.corrupt(src, rel, dst)\n",
        "            src_cuda = src.cuda()\n",
        "            rel_cuda = rel.cuda()\n",
        "            dst_cuda = dst.cuda()\n",
        "            src_corrupted = src_corrupted.cuda()\n",
        "            dst_corrupted = dst_corrupted.cuda()\n",
        "            for s0, r, t0, s1, t1 in batch_by_num(n_batch, src_cuda, rel_cuda, dst_cuda, src_corrupted, dst_corrupted,\n",
        "                                                  n_sample=n_train):\n",
        "\n",
        "                self.mdl.zero_grad()\n",
        "                loss = torch.sum(self.mdl.pair_loss(Variable(s0), Variable(r), Variable(t0), Variable(s1), Variable(t1)))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                self.mdl.constraint()\n",
        "                epoch_loss += loss.item()\n",
        "                #print('epoch_loss', epoch_loss)\n",
        "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
        "            if (epoch + 1) % self.config.epoch_per_test == 0:\n",
        "                test_perf = tester()\n",
        "                print('El MRR en epoch ',epoch,'fue de :', test_perf)\n",
        "                if test_perf > best_perf:\n",
        "                    task_dir = config().task.dir\n",
        "                    direccion=os.path.join(data_dir, task_dir)\n",
        "                    self.save(os.path.join(direccion, self.config.model_file))\n",
        "                    best_perf = test_perf\n",
        "        return best_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_lPB_Xuizcf4"
      },
      "outputs": [],
      "source": [
        "#trans_d.py\n",
        "class TransDModule(BaseModule):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(TransDModule, self).__init__()\n",
        "        self.margin = config.margin\n",
        "        self.p = config.p\n",
        "        self.temp = config.get('temp', 1)\n",
        "        self.rel_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.ent_embed = nn.Embedding(n_ent, config.dim)\n",
        "        self.proj_rel_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.proj_ent_embed = nn.Embedding(n_ent, config.dim)\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.normal_(1 / param.size(1) ** 0.5)\n",
        "            param.data.renorm_(2, 0, 1)\n",
        "\n",
        "    def forward(self, src, rel, dst):\n",
        "        src_proj = self.ent_embed(src) +\\\n",
        "                   torch.sum(self.proj_ent_embed(src) * self.ent_embed(src), dim=-1, keepdim=True) * self.proj_rel_embed(rel)\n",
        "        dst_proj = self.ent_embed(dst) +\\\n",
        "                   torch.sum(self.proj_ent_embed(dst) * self.ent_embed(dst), dim=-1, keepdim=True) * self.proj_rel_embed(rel)\n",
        "        return torch.norm(dst_proj - self.rel_embed(rel) - src_proj + 1e-30, p=self.p, dim=-1)\n",
        "\n",
        "    def dist(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "    def score(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "    def prob_logit(self, src, rel, dst):\n",
        "        return -self.forward(src, rel ,dst) / self.temp\n",
        "\n",
        "    def constraint(self):\n",
        "        for param in self.parameters():\n",
        "            param.data.renorm_(2, 0, 1)\n",
        "\n",
        "class TransD(BaseModel):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(TransD, self).__init__()\n",
        "        self.mdl = TransDModule(n_ent, n_rel, config)\n",
        "        self.mdl.cuda()\n",
        "        self.config = config\n",
        "\n",
        "    def pretrain(self, train_data, corrupter, tester):\n",
        "        print('estoy aqui en el pretrain de TransD')\n",
        "        src, rel, dst = train_data\n",
        "        n_train = len(src)\n",
        "        optimizer = Adam(self.mdl.parameters())\n",
        "        #optimizer = SGD(self.mdl.parameters(), lr=1e-4)\n",
        "        n_epoch = self.config.n_epoch\n",
        "        n_batch = self.config.n_batch\n",
        "        best_perf = 0\n",
        "        for epoch in range(n_epoch):\n",
        "            print('---------epoch_number---', epoch)\n",
        "            epoch_loss = 0\n",
        "            rand_idx = torch.randperm(n_train)\n",
        "            src = src[rand_idx]\n",
        "            rel = rel[rand_idx]\n",
        "            dst = dst[rand_idx]\n",
        "            src_corrupted, dst_corrupted = corrupter.corrupt(src, rel, dst)\n",
        "            src_cuda = src.cuda()\n",
        "            rel_cuda = rel.cuda()\n",
        "            dst_cuda = dst.cuda()\n",
        "            src_corrupted = src_corrupted.cuda()\n",
        "            dst_corrupted = dst_corrupted.cuda()\n",
        "            for s0, r, t0, s1, t1 in batch_by_num(n_batch, src_cuda, rel_cuda, dst_cuda, src_corrupted, dst_corrupted,\n",
        "                                                  n_sample=n_train):\n",
        "                self.mdl.zero_grad()\n",
        "                loss = torch.sum(self.mdl.pair_loss(Variable(s0), Variable(r), Variable(t0), Variable(s1), Variable(t1)))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                self.mdl.constraint()\n",
        "                epoch_loss += loss.item()\n",
        "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
        "            if (epoch + 1) % self.config.epoch_per_test == 0:\n",
        "                test_perf = tester()\n",
        "                print('El MRR en epoch ',epoch,'fue de :', test_perf)\n",
        "                if test_perf > best_perf:\n",
        "                    task_dir = config().task.dir\n",
        "                    direccion=os.path.join(data_dir, task_dir)\n",
        "                    self.save(os.path.join(direccion, self.config.model_file))\n",
        "                    best_perf = test_perf\n",
        "        return best_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "iMsrsJJ2HF9f"
      },
      "outputs": [],
      "source": [
        "#distmult.py\n",
        "class DistMultModule(BaseModule):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(DistMultModule, self).__init__()\n",
        "        sigma = 0.2\n",
        "        self.rel_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.rel_embed.weight.data.div_((config.dim / sigma ** 2) ** (1 / 6))\n",
        "        self.ent_embed = nn.Embedding(n_ent, config.dim)\n",
        "        self.ent_embed.weight.data.div_((config.dim / sigma ** 2) ** (1 / 6))\n",
        "\n",
        "    def forward(self, src, rel, dst):\n",
        "        return torch.sum(self.ent_embed(dst) * self.ent_embed(src) * self.rel_embed(rel), dim=-1)\n",
        "\n",
        "    def score(self, src, rel, dst):\n",
        "        return -self.forward(src, rel, dst)\n",
        "\n",
        "    def dist(self, src, rel, dst):\n",
        "        return -self.forward(src, rel, dst)\n",
        "\n",
        "    def prob_logit(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "class DistMult(BaseModel):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(DistMult, self).__init__()\n",
        "        self.mdl = DistMultModule(n_ent, n_rel, config)\n",
        "        self.mdl.cuda()\n",
        "        self.config = config\n",
        "        self.weight_decay = config.lam / config.n_batch\n",
        "\n",
        "    def pretrain(self, train_data, corrupter, tester):\n",
        "        src, rel, dst = train_data\n",
        "        n_train = len(src)\n",
        "        n_epoch = self.config.n_epoch\n",
        "        n_batch = self.config.n_batch\n",
        "        optimizer = Adam(self.mdl.parameters(), weight_decay=self.weight_decay)\n",
        "        best_perf = 0\n",
        "        for epoch in range(n_epoch):\n",
        "            print('---------epoch_number---', epoch)\n",
        "            epoch_loss = 0\n",
        "            if epoch % self.config.sample_freq == 0:\n",
        "                rand_idx = torch.randperm(n_train)\n",
        "                src = src[rand_idx]\n",
        "                rel = rel[rand_idx]\n",
        "                dst = dst[rand_idx]\n",
        "                src_corrupted, rel_corrupted, dst_corrupted = corrupter.corrupt(src, rel, dst)\n",
        "                src_corrupted = src_corrupted.cuda()\n",
        "                rel_corrupted = rel_corrupted.cuda()\n",
        "                dst_corrupted = dst_corrupted.cuda()\n",
        "            for ss, rs, ts in batch_by_num(n_batch, src_corrupted, rel_corrupted, dst_corrupted, n_sample=n_train):\n",
        "                self.mdl.zero_grad()\n",
        "                label = torch.zeros(len(ss)).type(torch.LongTensor).cuda()\n",
        "                loss = torch.sum(self.mdl.softmax_loss(Variable(ss), Variable(rs), Variable(ts), label))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
        "            if (epoch + 1) % self.config.epoch_per_test == 0:\n",
        "                test_perf = tester()\n",
        "                print('El MRR en epoch ',epoch,'fue de :', test_perf)\n",
        "                if test_perf > best_perf:\n",
        "                    task_dir = config().task.dir\n",
        "                    direccion=os.path.join(data_dir, task_dir)\n",
        "                    self.save(os.path.join(direccion, self.config.model_file))\n",
        "                    best_perf = test_perf\n",
        "        return best_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "III29HhjpiL3"
      },
      "outputs": [],
      "source": [
        "class ComplExModule(BaseModule):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(ComplExModule, self).__init__()\n",
        "        sigma = 0.2\n",
        "        self.rel_re_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.rel_im_embed = nn.Embedding(n_rel, config.dim)\n",
        "        self.ent_re_embed = nn.Embedding(n_ent, config.dim)\n",
        "        self.ent_im_embed = nn.Embedding(n_ent, config.dim)\n",
        "        for param in self.parameters():\n",
        "            param.data.div_((config.dim / sigma ** 2) ** (1 / 6))\n",
        "\n",
        "    def forward(self, src, rel, dst):\n",
        "        return torch.sum(self.rel_re_embed(rel) * self.ent_re_embed(src) * self.ent_re_embed(dst), dim=-1) \\\n",
        "            + torch.sum(self.rel_re_embed(rel) * self.ent_im_embed(src) * self.ent_im_embed(dst), dim=-1) \\\n",
        "            + torch.sum(self.rel_im_embed(rel) * self.ent_re_embed(src) * self.ent_im_embed(dst), dim=-1) \\\n",
        "            - torch.sum(self.rel_im_embed(rel) * self.ent_im_embed(src) * self.ent_re_embed(dst), dim=-1)\n",
        "\n",
        "    def score(self, src, rel, dst):\n",
        "        return -self.forward(src, rel, dst)\n",
        "\n",
        "    def dist(self, src, rel, dst):\n",
        "        return -self.forward(src, rel, dst)\n",
        "\n",
        "    def prob_logit(self, src, rel, dst):\n",
        "        return self.forward(src, rel, dst)\n",
        "\n",
        "class ComplEx(BaseModel):\n",
        "    def __init__(self, n_ent, n_rel, config):\n",
        "        super(ComplEx, self).__init__()\n",
        "        self.mdl = ComplExModule(n_ent, n_rel, config)\n",
        "        self.mdl.cuda()\n",
        "        self.config = config\n",
        "        self.weight_decay = config.lam / config.n_batch\n",
        "\n",
        "    def pretrain(self, train_data, corrupter, tester):\n",
        "        src, rel, dst = train_data\n",
        "        n_train = len(src)\n",
        "        n_epoch = self.config.n_epoch\n",
        "        n_batch = self.config.n_batch\n",
        "        optimizer = Adam(self.mdl.parameters(), weight_decay=self.weight_decay)\n",
        "        best_perf = 0\n",
        "        for epoch in range(n_epoch):\n",
        "            print('---------epoch_number---', epoch)\n",
        "            epoch_loss = 0\n",
        "            if epoch % self.config.sample_freq == 0:\n",
        "                rand_idx = torch.randperm(n_train)\n",
        "                src = src[rand_idx]\n",
        "                rel = rel[rand_idx]\n",
        "                dst = dst[rand_idx]\n",
        "                src_corrupted, rel_corrupted, dst_corrupted = corrupter.corrupt(src, rel, dst)\n",
        "                src_corrupted = src_corrupted.cuda()\n",
        "                rel_corrupted = rel_corrupted.cuda()\n",
        "                dst_corrupted = dst_corrupted.cuda()\n",
        "            for ss, rs, ts in batch_by_num(n_batch, src_corrupted, rel_corrupted, dst_corrupted, n_sample=n_train):\n",
        "                self.mdl.zero_grad()\n",
        "                label = torch.zeros(len(ss)).type(torch.LongTensor).cuda()\n",
        "                loss = torch.sum(self.mdl.softmax_loss(Variable(ss), Variable(rs), Variable(ts), label))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
        "            if (epoch + 1) % self.config.epoch_per_test == 0:\n",
        "                test_perf = tester()\n",
        "                print('El MRR en epoch ',epoch,'fue de :', test_perf)\n",
        "                if test_perf > best_perf:\n",
        "                    task_dir = config().task.dir\n",
        "                    direccion=os.path.join(data_dir, task_dir)\n",
        "                    self.save(os.path.join(direccion, self.config.model_file))\n",
        "                    best_perf = test_perf\n",
        "        return best_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OsOL03pqC_cF"
      },
      "outputs": [],
      "source": [
        "#pretrain.py\n",
        "def pretrain():\n",
        "    task_dir = config().task.dir\n",
        "    direccion=os.path.join(data_dir, task_dir)\n",
        "    kb_index = index_ent_rel(os.path.join(direccion, 'train.txt'),\n",
        "                        os.path.join(direccion, 'valid.txt'),\n",
        "                        os.path.join(direccion, 'test.txt'))\n",
        "\n",
        "    n_ent, n_rel = graph_size(kb_index)\n",
        "    train_data = read_data(os.path.join(direccion, 'train.txt'), kb_index)\n",
        "    inplace_shuffle(*train_data)\n",
        "    valid_data = read_data(os.path.join(direccion, 'valid.txt'), kb_index)\n",
        "    test_data = read_data(os.path.join(direccion, 'test.txt'), kb_index)\n",
        "\n",
        "    heads, tails = heads_tails(n_ent, train_data, valid_data, test_data)\n",
        "    valid_data = [torch.LongTensor(vec) for vec in valid_data]\n",
        "    test_data = [torch.LongTensor(vec) for vec in test_data]\n",
        "    tester = lambda: gen.test_link(valid_data, n_ent, heads, tails)\n",
        "    train_data = [torch.LongTensor(vec) for vec in train_data]\n",
        "    mdl_type = config().pretrain_config\n",
        "    gen_config = config()[mdl_type]\n",
        "    if mdl_type == 'TransE':\n",
        "        corrupter = BernCorrupter(train_data, n_ent, n_rel)\n",
        "        gen = TransE(n_ent, n_rel, gen_config)\n",
        "    elif mdl_type == 'TransD':\n",
        "        corrupter = BernCorrupter(train_data, n_ent, n_rel)\n",
        "        gen = TransD(n_ent, n_rel, gen_config)\n",
        "    elif mdl_type == 'DistMult':\n",
        "        corrupter = BernCorrupterMulti(train_data, n_ent, n_rel, gen_config.n_sample)\n",
        "        gen = DistMult(n_ent, n_rel, gen_config)\n",
        "    elif mdl_type == 'ComplEx':\n",
        "        corrupter = BernCorrupterMulti(train_data, n_ent, n_rel, gen_config.n_sample)\n",
        "        gen = ComplEx(n_ent, n_rel, gen_config)\n",
        "    print(\"Comenzando el entrenamiento con el modelo:\", mdl_type)\n",
        "    result=gen.pretrain(train_data, corrupter, tester)\n",
        "    print('El best MRR fue de :', result)\n",
        "    print('Cargando modelo preentrenado')\n",
        "    gen.load(os.path.join(direccion, gen_config.model_file))\n",
        "    print('Ejecutando pruebas')\n",
        "    result=gen.test_link(test_data, n_ent, heads, tails)\n",
        "    print('El MRR para el conjunto de pruebas en el modelo entrenado con :', gen_config.n_epoch,'epochs es de: ',result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3dneGeqpVDA0"
      },
      "outputs": [],
      "source": [
        "#gan_train.py\n",
        "def gan_train():\n",
        "    task_dir = config().task.dir\n",
        "    direccion=os.path.join(data_dir, task_dir)\n",
        "    kb_index = index_ent_rel(os.path.join(direccion, 'train.txt'),\n",
        "                             os.path.join(direccion, 'valid.txt'),\n",
        "                             os.path.join(direccion, 'test.txt'))\n",
        "    n_ent, n_rel = graph_size(kb_index)\n",
        "\n",
        "\n",
        "    models = {'TransE': TransE, 'TransD': TransD, 'DistMult': DistMult, 'ComplEx': ComplEx}\n",
        "    gen_config = config()[config().g_config]\n",
        "    dis_config = config()[config().d_config]\n",
        "    gen = models[config().g_config](n_ent, n_rel, gen_config)\n",
        "    dis = models[config().d_config](n_ent, n_rel, dis_config)\n",
        "    gen.load(os.path.join(direccion, gen_config.model_file))\n",
        "    dis.load(os.path.join(direccion, dis_config.model_file))\n",
        "    train_data = read_data(os.path.join(direccion, 'train.txt'), kb_index)\n",
        "    inplace_shuffle(*train_data)\n",
        "    valid_data = read_data(os.path.join(direccion, 'valid.txt'), kb_index)\n",
        "    test_data = read_data(os.path.join(direccion, 'test.txt'), kb_index)\n",
        "    filt_heads, filt_tails = heads_tails(n_ent, train_data, valid_data, test_data)\n",
        "    valid_data = [torch.LongTensor(vec) for vec in valid_data]\n",
        "    test_data = [torch.LongTensor(vec) for vec in test_data]\n",
        "    tester = lambda: dis.test_link(valid_data, n_ent, filt_heads, filt_tails)\n",
        "    train_data = [torch.LongTensor(vec) for vec in train_data]\n",
        "\n",
        "    mrr_result=dis.test_link(test_data, n_ent, filt_heads, filt_tails)\n",
        "    print('El MRR del modelo discriminador antes de ser entrenado con GAN para test_data es de :',mrr_result)\n",
        "\n",
        "    corrupter = BernCorrupterMulti(train_data, n_ent, n_rel, config().adv.n_sample)\n",
        "    src, rel, dst = train_data\n",
        "    n_train = len(src)\n",
        "    n_epoch = config().adv.n_epoch\n",
        "    n_batch = config().adv.n_batch\n",
        "    mdl_name = 'gan_dis_' + datetime.datetime.now().strftime(\"%m%d%H%M%S\") + '.mdl'\n",
        "    best_perf = 0\n",
        "    avg_reward = 0\n",
        "    for epoch in range(n_epoch):\n",
        "        epoch_d_loss = 0\n",
        "        epoch_reward = 0\n",
        "        src_cand, rel_cand, dst_cand = corrupter.corrupt(src, rel, dst, keep_truth=False)\n",
        "        for _s, _r, _t, ss, rs, ts in batch_by_num(n_batch, src, rel, dst, src_cand, rel_cand, dst_cand, n_sample=n_train):\n",
        "            gen_step = gen.gen_step(ss, rs, ts, temperature=config().adv.temperature)\n",
        "            src_smpl, dst_smpl = next(gen_step)\n",
        "            pdb.set_trace()\n",
        "            losses, rewards = dis.dis_step(_s, _r, _t, src_smpl.squeeze(), dst_smpl.squeeze())\n",
        "            epoch_reward += torch.sum(rewards)\n",
        "            rewards = rewards - avg_reward\n",
        "            gen_step.send(rewards.unsqueeze(1))\n",
        "            epoch_d_loss += torch.sum(losses)\n",
        "            #print('epoch_loss', epoch_d_loss.item())\n",
        "            #print('recompensa', torch.sum(rewards).item())\n",
        "        avg_loss = epoch_d_loss / n_train\n",
        "        avg_reward = epoch_reward / n_train\n",
        "        logging.info('Epoch %d/%d, D_loss=%f, reward=%f', epoch + 1, n_epoch, avg_loss, avg_reward)\n",
        "        if (epoch + 1) % config().adv.epoch_per_test == 0:\n",
        "            #gen.test_link(valid_data, n_ent, filt_heads, filt_tails)\n",
        "            perf = dis.test_link(valid_data, n_ent, filt_heads, filt_tails)\n",
        "            print('El MRR en epoch ',epoch,'para valid_data fue de :', perf)\n",
        "            if perf > best_perf:\n",
        "                best_perf = perf\n",
        "                task_dir = config().task.dir\n",
        "                direccion=os.path.join(data_dir, task_dir)\n",
        "                dis.save(os.path.join(direccion, mdl_name))\n",
        "    dis.load(os.path.join(config().task.dir, mdl_name))\n",
        "    mrr_result=dis.test_link(test_data, n_ent, filt_heads, filt_tails)\n",
        "    print('El MRR del modelo discriminador despues de ser entrenado con GAN para test_data es de :',mrr_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dazvUX91UxoQ",
        "outputId": "f8945da3-d48f-4400-bf22-cbb4e8de6aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/m/0125ny\t/soccer/football_team/current_roster./sports/sports_team_roster/position\t/m/01hbq0\n",
            "/m/011hq1\t/film/film/story_by\t/m/013rds\n",
            "/m/0102t4\t/education/university/international_tuition./measurement_unit/dated_money_value/currency\t/m/0100mt\n",
            "/m/0np9r\t/film/film/dubbing_performances./film/dubbing_performance/actor\t/m/0127m7\n",
            "100\t200\t1677\n",
            "32\t98\t290\n",
            "2\t65\t1\n",
            "13890\t76\t111\n",
            "Fin\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    #pretrain()\n",
        "    #gan_train()\n",
        "    #tripletas_by_identificadores()\n",
        "    #tripletas_by_texto()\n",
        "    print('Fin')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN48FaCa8i8XOjl6URAEf4a",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}